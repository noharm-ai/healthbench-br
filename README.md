# HealthBench-BR ğŸ‡§ğŸ‡·

**Dataset de avaliaÃ§Ã£o para Large Language Models (LLMs) em PortuguÃªs do Brasil na Ã¡rea da saÃºde**

## ğŸ“‹ DescriÃ§Ã£o

O HealthBench-BR Ã© um benchmark especializado para avaliar a performance de modelos de linguagem em questÃµes mÃ©dicas e de saÃºde em portuguÃªs brasileiro. Utiliza um conjunto de perguntas Verdadeiro/Falso cuidadosamente elaboradas para testar o conhecimento e raciocÃ­nio dos modelos em contextos mÃ©dicos.

### ğŸ¯ CaracterÃ­sticas Principais
- **Arquitetura Modular**: Sistema componentizado com suporte a mÃºltiplos provedores LLM
- **IntegraÃ§Ã£o LangChain**: Utiliza LangChain para abstraÃ§Ã£o e padronizaÃ§Ã£o
- **Multi-Provider**: Suporte para OpenAI, Maritaca/SabiÃ¡, Ollama e AWS Bedrock
- **AvaliaÃ§Ã£o AssÃ­ncrona**: Processamento paralelo para maior eficiÃªncia
- **RelatÃ³rios Detalhados**: ExportaÃ§Ã£o em CSV e JSON com mÃ©tricas completas

## ğŸ“Š Exemplos de Perguntas

Aqui estÃ£o dois exemplos de perguntas do benchmark, com suas respostas esperadas:

**Pergunta 1 (Verdadeiro):**
"No que diz respeito ao protocolo clÃ­nico \"Protocolo ClÃ­nico e Diretrizes TerapÃªuticas da Vasculite Associada aos Anticorpos Anti-citoplasma de NeutrÃ³filos\", a afirmaÃ§Ã£o a seguir Ã© verdadeira ou falsa? No Brasil, hÃ¡ carÃªncia de investigaÃ§Ãµes epidemiolÃ³gicas detalhadas sobre incidÃªncia e prevalÃªncia da vasculite ANCA-associada, o que reforÃ§a a necessidade de estudos para estruturar polÃ­ticas de saÃºde alinhadas Ã s especificidades regionais."

**Resposta esperada:** Verdadeiro

**Pergunta 2 (Falso):**
"No que diz respeito ao protocolo clÃ­nico \"Protocolo ClÃ­nico e Diretrizes TerapÃªuticas da Vasculite Associada aos Anticorpos Anti-citoplasma de NeutrÃ³filos\", a afirmaÃ§Ã£o a seguir Ã© verdadeira ou falsa? No Brasil, hÃ¡ estudos abrangentes e consolidados que mapeiam com precisÃ£o a incidÃªncia e a prevalÃªncia da vasculite ANCA-associada, permitindo estruturar polÃ­ticas de saÃºde jÃ¡ plenamente alinhadas Ã s especificidades regionais."

**Resposta esperada:** Falso

## ğŸ“ˆ Resultados de Benchmark

Abaixo estÃ£o os resultados de avaliaÃ§Ã£o dos principais modelos LLM no HealthBench-BR, utilizando as primeiras 50 perguntas do dataset:

| Provider              | Tipo      | Modelo         | Total | Acertos | AcurÃ¡cia | Tempo (s) |
|-----------------------|-----------|----------------|-------|---------|----------|-----------|
| GPT-4.1              | openai   | gpt-4.1       | 50   | 35     | 70.00%  | 55.7     |
| GPT-5                | openai   | gpt-5         | 50   | 42     | 84.00%  | 398.0    |
| Maritaca-Sabiazinho-3| maritaca | sabiazinho-3  | 50   | 30     | 60.00%  | 13.3     |
| Maritaca-Sabia-3.1   | maritaca | sabia-3.1     | 50   | 36     | 72.00%  | 37.1     |

ğŸ† **Melhor desempenho:** GPT-5 com 84.00% de acurÃ¡cia.

## ğŸ—ï¸ Estrutura do Projeto

```
healthbench-br/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ providers/          # ImplementaÃ§Ãµes dos provedores LLM
â”‚   â”‚   â”œâ”€â”€ base.py         # Classe base abstrata
â”‚   â”‚   â”œâ”€â”€ maritaca.py     # Provider Maritaca/SabiÃ¡
â”‚   â”‚   â”œâ”€â”€ openai_provider.py  # Provider OpenAI
â”‚   â”‚   â”œâ”€â”€ ollama.py       # Provider Ollama
â”‚   â”‚   â””â”€â”€ bedrock.py      # Provider AWS Bedrock
â”‚   â”œâ”€â”€ dataset/            # Carregamento e parsing de dados
â”‚   â”‚   â””â”€â”€ loader.py       # DatasetLoader e ResponseParser
â”‚   â”œâ”€â”€ evaluation/         # LÃ³gica de avaliaÃ§Ã£o
â”‚   â”‚   â””â”€â”€ evaluator.py    # Classe Evaluator principal
â”‚   â””â”€â”€ reports/            # GeraÃ§Ã£o de relatÃ³rios
â”‚       â””â”€â”€ generator.py    # ReportGenerator
â”œâ”€â”€ evaluate_batch.py       # AvaliaÃ§Ã£o em lote (mÃºltiplos LLMs)
â”œâ”€â”€ evaluate.py             # AvaliaÃ§Ã£o individual (um LLM)
â”œâ”€â”€ requirements.txt        # DependÃªncias do projeto
â””â”€â”€ benchmark_perguntas_unificado.json  # Dataset de perguntas
```

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos
- Python 3.8+
- pip

### InstalaÃ§Ã£o das DependÃªncias

```bash
pip install -r requirements.txt
```

### DependÃªncias Principais
- `langchain` - Framework principal para LLMs
- `langchain-openai` - IntegraÃ§Ã£o com OpenAI e compatÃ­veis
- `langchain-community` - Provedores da comunidade (Ollama)
- `langchain-aws` - IntegraÃ§Ã£o com AWS Bedrock
- `openai` - Cliente OpenAI
- `boto3` - SDK AWS (para Bedrock)
- `tqdm` - Barras de progresso
- `aiohttp` - Cliente HTTP assÃ­ncrono

## ğŸ“Š Dataset

### Formato do Arquivo
O dataset deve estar em formato JSON com a seguinte estrutura:

**`benchmark_perguntas_unificado.json`**
```json
[
  {
    "arquivo": "nome_do_arquivo.txt",
    "titulo": "TÃ­tulo do Conjunto de Perguntas",
    "perguntas": [
      "Pergunta que deve ser respondida como Verdadeiro",
      "Pergunta que deve ser respondida como Falso",
      "Outra pergunta Verdadeiro",
      "Outra pergunta Falso"
    ]
  }
]
```

**Importante**: As perguntas devem estar em pares sequenciais:
- PosiÃ§Ã£o par (0, 2, 4...) = **Verdadeiro**
- PosiÃ§Ã£o Ã­mpar (1, 3, 5...) = **Falso**

## ğŸ’» Uso

O HealthBench-BR oferece duas ferramentas de avaliaÃ§Ã£o:

### ğŸ”„ AvaliaÃ§Ã£o em Lote (Recomendado)

Para avaliar mÃºltiplos LLMs simultaneamente usando configuraÃ§Ã£o centralizada:

```bash
python evaluate_batch.py [opÃ§Ãµes]
```

#### ParÃ¢metros do Batch
- `--config`: Arquivo de configuraÃ§Ã£o (padrÃ£o: `providers.json`)
- `--dataset`: Dataset a usar (padrÃ£o: `benchmark_perguntas_unificado.json`)  
- `--providers`: Lista de providers especÃ­ficos para avaliar
- `--limit`: Limitar nÃºmero de perguntas por provider
- `--output-dir`: DiretÃ³rio de saÃ­da (padrÃ£o: `evaluation_results`)
- `--no-progress`: Desabilitar barras de progresso

#### Exemplos de AvaliaÃ§Ã£o em Lote

**Avaliar todos os providers ativos:**
```bash
python evaluate_batch.py
```

**Avaliar providers especÃ­ficos:**
```bash
python evaluate_batch.py --providers "MedGemma-27B-Q8" "Claude-Sonnet-4-Bedrock"
```

**Com limite de perguntas:**
```bash
python evaluate_batch.py --limit 50 --output-dir my_results
```

**PrÃ©-requisito**: Configure o arquivo `providers.json` com os LLMs desejados. Exemplo:
```json
{
  "providers": [
    {
      "name": "Claude-Sonnet-4-Bedrock",
      "type": "aws_bedrock", 
      "model": "global.anthropic.claude-sonnet-4-20250514-v1:0",
      "region": "us-east-1",
      "aws_bearer_token": "${AWS_BEARER_TOKEN_BEDROCK}",
      "temperature": 0.0,
      "max_tokens": 12000,
      "active": true
    }
  ],
  "default_settings": {
    "parallelism": 10,
    "timeout": 120
  }
}
```

### ğŸ¯ AvaliaÃ§Ã£o Individual

Para avaliar um Ãºnico LLM por vez:

```bash
python evaluate.py --provider PROVIDER --model MODEL [opÃ§Ãµes]
```

### ParÃ¢metros Principais

#### ObrigatÃ³rios
- `--provider`: Provedor LLM (`maritaca`, `openai`, `ollama`, `bedrock`)
- `--model`: Nome do modelo a usar

#### ConfiguraÃ§Ã£o do Modelo
- `--temperature`: Temperatura da inferÃªncia (padrÃ£o: 0.0)
- `--max_tokens`: MÃ¡ximo de tokens por resposta (padrÃ£o: 12000)
- `--timeout`: Timeout em segundos (padrÃ£o: 120)

#### ConfiguraÃ§Ã£o da API
- `--api_key`: Chave de API (obrigatÃ³rio para OpenAI/Maritaca)
- `--base_url`: URL base customizada (opcional)

#### AWS Bedrock
- `--aws_access_key_id`: AWS Access Key ID
- `--aws_secret_access_key`: AWS Secret Access Key
- `--aws_session_token`: AWS Session Token (opcional)
- `--aws_region`: RegiÃ£o AWS (padrÃ£o: us-east-1)

#### Dataset e AvaliaÃ§Ã£o
- `--dataset_path`: Caminho para o arquivo JSON (padrÃ£o: benchmark_perguntas_unificado.json)
- `--limit`: Limitar nÃºmero de perguntas a avaliar
- `--parallelism`: NÃºmero de chamadas paralelas (padrÃ£o: 10)

#### SaÃ­da
- `--csv_out`: Nome do arquivo CSV de saÃ­da (padrÃ£o: resultados_avaliacao.csv)
- `--detailed_report`: Gerar relatÃ³rio detalhado em JSON
- `--no_progress`: Desabilitar barra de progresso

## ğŸ“ Exemplos de Uso

### OpenAI (GPT-4)
```bash
python evaluate.py \
  --provider openai \
  --model gpt-4 \
  --api_key sk-... \
  --temperature 0.1 \
  --parallelism 5
```

### Maritaca/SabiÃ¡
```bash
python evaluate.py \
  --provider maritaca \
  --model sabia-3 \
  --api_key sua_chave_maritaca \
  --base_url https://api.maritaca.ai/v1 \
  --limit 100
```

### Ollama (Local)
```bash
python evaluate.py \
  --provider ollama \
  --model llama3.2:3b \
  --base_url http://localhost:11434 \
  --parallelism 20
```

### AWS Bedrock (Claude)
```bash
python evaluate.py \
  --provider bedrock \
  --model claude-3-sonnet \
  --aws_access_key_id AKIAIOSFODNN7EXAMPLE \
  --aws_secret_access_key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \
  --aws_region us-east-1 \
  --detailed_report
```

### AvaliaÃ§Ã£o com RelatÃ³rio Detalhado
```bash
python evaluate.py \
  --provider openai \
  --model gpt-3.5-turbo \
  --api_key sk-... \
  --dataset_path meu_dataset.json \
  --csv_out resultados_gpt35.csv \
  --detailed_report \
  --limit 50 \
  --parallelism 10
```

## ğŸ” Formato de Resposta Esperado

O modelo deve responder seguindo o formato:
```
[ExplicaÃ§Ã£o opcional do modelo]

Resposta: Verdadeiro
```
ou
```
[ExplicaÃ§Ã£o opcional do modelo]

Resposta: Falso
```

O sistema extrai automaticamente a Ãºltima ocorrÃªncia de "Verdadeiro" ou "Falso" na resposta.

## ğŸ“¤ SaÃ­das

### 1. Console
- Progresso em tempo real com barra de progresso
- AcurÃ¡cia parcial durante execuÃ§Ã£o
- Resumo final com mÃ©tricas completas

### 2. Arquivo CSV (`resultados_avaliacao.csv`)
ContÃ©m as seguintes colunas:
- `arquivo`: Nome do arquivo fonte
- `titulo`: TÃ­tulo do conjunto
- `idx_local`: Ãndice da pergunta
- `pergunta`: Texto da pergunta
- `esperado`: Resposta correta
- `pred`: PrediÃ§Ã£o do modelo
- `correta`: 1 se acertou, 0 se errou
- `resposta_bruta`: Resposta completa do modelo

### 3. RelatÃ³rio JSON Detalhado (opcional)
Quando usar `--detailed_report`, gera arquivo JSON com:
- Timestamp da avaliaÃ§Ã£o
- MÃ©tricas agregadas
- MÃ©tricas por arquivo
- MÃ©tricas por tÃ­tulo
- Resultados completos de cada pergunta

## âš¡ ConfiguraÃ§Ã£o de Performance

### Paralelismo
Ajuste `--parallelism` baseado no provider:
- **OpenAI/Maritaca**: 5-10 (respeitar rate limits)
- **Ollama (local)**: 20-50 (depende do hardware)
- **AWS Bedrock**: 10-20 (verificar cotas da regiÃ£o)

### Timeout
Ajuste `--timeout` para modelos mais lentos ou prompts complexos

## ğŸ”§ Troubleshooting

### Erro de ImportaÃ§Ã£o
```bash
ImportError: No module named 'langchain'
```
**SoluÃ§Ã£o**: Instalar dependÃªncias
```bash
pip install -r requirements.txt
```

### Erro de API Key
```bash
Erro: --api_key Ã© obrigatÃ³rio para o provider openai
```
**SoluÃ§Ã£o**: Fornecer a chave de API apropriada

### Timeout em Modelos Locais
**SoluÃ§Ã£o**: Aumentar o timeout
```bash
--timeout 300  # 5 minutos
```

### Rate Limit Exceeded
**SoluÃ§Ã£o**: Reduzir paralelismo
```bash
--parallelism 3
```

## ğŸ¤ ContribuiÃ§Ã£o

Para contribuir com o projeto:

1. Adicionar novos providers em `src/providers/`
2. Estender a classe `BaseLLMProvider`
3. Implementar o mÃ©todo `initialize()`
4. Atualizar `evaluate.py` para incluir o novo provider
5. Adicionar dependÃªncias em `requirements.txt`
6. Documentar uso no README

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a especificada no arquivo [LICENSE](LICENSE).

## ğŸ”„ MigraÃ§Ã£o do Script Antigo

Se vocÃª estava usando o script `evaluate_py.py` anterior, migre para o novo sistema:

**Antes (antigo):**
```bash
python evaluate_py.py \
  --model sabia3.1 \
  --base_url https://api.maritaca.ai/v1 \
  --api_key KEY
```

**Agora (novo):**
```bash
python evaluate.py \
  --provider maritaca \
  --model sabia3.1 \
  --api_key KEY
```

### Principais Melhorias
- âœ… Suporte a mÃºltiplos providers
- âœ… Arquitetura modular e extensÃ­vel
- âœ… IntegraÃ§Ã£o com LangChain
- âœ… Melhor tratamento de erros
- âœ… RelatÃ³rios mais detalhados
- âœ… CÃ³digo mais limpo e manutenÃ­vel

## ğŸ‘¥ Autores

Desenvolvido para avaliaÃ§Ã£o de LLMs em portuguÃªs brasileiro no contexto mÃ©dico e de saÃºde.