# HealthBench-BR ğŸ‡§ğŸ‡·

**Dataset de avaliaÃ§Ã£o para Large Language Models (LLMs) em PortuguÃªs do Brasil na Ã¡rea da saÃºde**

## ğŸ“‹ DescriÃ§Ã£o

O HealthBench-BR Ã© um benchmark especializado para avaliar a performance de modelos de linguagem em questÃµes mÃ©dicas e de saÃºde em portuguÃªs brasileiro. Utiliza um conjunto de perguntas Verdadeiro/Falso cuidadosamente elaboradas para testar o conhecimento e raciocÃ­nio dos modelos em contextos mÃ©dicos.

### ğŸ¯ CaracterÃ­sticas Principais
- **Arquitetura Modular**: Sistema componentizado com suporte a mÃºltiplos provedores LLM
- **IntegraÃ§Ã£o LangChain**: Utiliza LangChain para abstraÃ§Ã£o e padronizaÃ§Ã£o
- **Multi-Provider**: Suporte para OpenAI, Maritaca/SabiÃ¡, Ollama e AWS Bedrock
- **AvaliaÃ§Ã£o AssÃ­ncrona**: Processamento paralelo para maior eficiÃªncia
- **RelatÃ³rios Detalhados**: ExportaÃ§Ã£o em CSV e JSON com mÃ©tricas completas

## ğŸ—ï¸ Estrutura do Projeto

```
healthbench-br/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ providers/          # ImplementaÃ§Ãµes dos provedores LLM
â”‚   â”‚   â”œâ”€â”€ base.py         # Classe base abstrata
â”‚   â”‚   â”œâ”€â”€ maritaca.py     # Provider Maritaca/SabiÃ¡
â”‚   â”‚   â”œâ”€â”€ openai_provider.py  # Provider OpenAI
â”‚   â”‚   â”œâ”€â”€ ollama.py       # Provider Ollama
â”‚   â”‚   â””â”€â”€ bedrock.py      # Provider AWS Bedrock
â”‚   â”œâ”€â”€ dataset/            # Carregamento e parsing de dados
â”‚   â”‚   â””â”€â”€ loader.py       # DatasetLoader e ResponseParser
â”‚   â”œâ”€â”€ evaluation/         # LÃ³gica de avaliaÃ§Ã£o
â”‚   â”‚   â””â”€â”€ evaluator.py    # Classe Evaluator principal
â”‚   â””â”€â”€ reports/            # GeraÃ§Ã£o de relatÃ³rios
â”‚       â””â”€â”€ generator.py    # ReportGenerator
â”œâ”€â”€ evaluate.py             # Script principal
â”œâ”€â”€ evaluate_py.py          # Script legado (mantido para compatibilidade)
â”œâ”€â”€ requirements.txt        # DependÃªncias do projeto
â””â”€â”€ benchmark_perguntas_unificado.json  # Dataset de perguntas
```

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos
- Python 3.8+
- pip

### InstalaÃ§Ã£o das DependÃªncias

```bash
pip install -r requirements.txt
```

### DependÃªncias Principais
- `langchain` - Framework principal para LLMs
- `langchain-openai` - IntegraÃ§Ã£o com OpenAI e compatÃ­veis
- `langchain-community` - Provedores da comunidade (Ollama)
- `langchain-aws` - IntegraÃ§Ã£o com AWS Bedrock
- `openai` - Cliente OpenAI
- `boto3` - SDK AWS (para Bedrock)
- `tqdm` - Barras de progresso
- `aiohttp` - Cliente HTTP assÃ­ncrono

## ğŸ“Š Dataset

### Formato do Arquivo
O dataset deve estar em formato JSON com a seguinte estrutura:

**`benchmark_perguntas_unificado.json`**
```json
[
  {
    "arquivo": "nome_do_arquivo.txt",
    "titulo": "TÃ­tulo do Conjunto de Perguntas",
    "perguntas": [
      "Pergunta que deve ser respondida como Verdadeiro",
      "Pergunta que deve ser respondida como Falso",
      "Outra pergunta Verdadeiro",
      "Outra pergunta Falso"
    ]
  }
]
```

**Importante**: As perguntas devem estar em pares sequenciais:
- PosiÃ§Ã£o par (0, 2, 4...) = **Verdadeiro**
- PosiÃ§Ã£o Ã­mpar (1, 3, 5...) = **Falso**

## ğŸ’» Uso

### Comando BÃ¡sico

```bash
python evaluate.py --provider PROVIDER --model MODEL [opÃ§Ãµes]
```

### ParÃ¢metros Principais

#### ObrigatÃ³rios
- `--provider`: Provedor LLM (`maritaca`, `openai`, `ollama`, `bedrock`)
- `--model`: Nome do modelo a usar

#### ConfiguraÃ§Ã£o do Modelo
- `--temperature`: Temperatura da inferÃªncia (padrÃ£o: 0.0)
- `--max_tokens`: MÃ¡ximo de tokens por resposta (padrÃ£o: 12000)
- `--timeout`: Timeout em segundos (padrÃ£o: 120)

#### ConfiguraÃ§Ã£o da API
- `--api_key`: Chave de API (obrigatÃ³rio para OpenAI/Maritaca)
- `--base_url`: URL base customizada (opcional)

#### AWS Bedrock
- `--aws_access_key_id`: AWS Access Key ID
- `--aws_secret_access_key`: AWS Secret Access Key
- `--aws_session_token`: AWS Session Token (opcional)
- `--aws_region`: RegiÃ£o AWS (padrÃ£o: us-east-1)

#### Dataset e AvaliaÃ§Ã£o
- `--dataset_path`: Caminho para o arquivo JSON (padrÃ£o: benchmark_perguntas_unificado.json)
- `--limit`: Limitar nÃºmero de perguntas a avaliar
- `--parallelism`: NÃºmero de chamadas paralelas (padrÃ£o: 10)

#### SaÃ­da
- `--csv_out`: Nome do arquivo CSV de saÃ­da (padrÃ£o: resultados_avaliacao.csv)
- `--detailed_report`: Gerar relatÃ³rio detalhado em JSON
- `--no_progress`: Desabilitar barra de progresso

## ğŸ“ Exemplos de Uso

### OpenAI (GPT-4)
```bash
python evaluate.py \
  --provider openai \
  --model gpt-4 \
  --api_key sk-... \
  --temperature 0.1 \
  --parallelism 5
```

### Maritaca/SabiÃ¡
```bash
python evaluate.py \
  --provider maritaca \
  --model sabia-3 \
  --api_key sua_chave_maritaca \
  --base_url https://api.maritaca.ai/v1 \
  --limit 100
```

### Ollama (Local)
```bash
python evaluate.py \
  --provider ollama \
  --model llama3.2:3b \
  --base_url http://localhost:11434 \
  --parallelism 20
```

### AWS Bedrock (Claude)
```bash
python evaluate.py \
  --provider bedrock \
  --model claude-3-sonnet \
  --aws_access_key_id AKIAIOSFODNN7EXAMPLE \
  --aws_secret_access_key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \
  --aws_region us-east-1 \
  --detailed_report
```

### AvaliaÃ§Ã£o com RelatÃ³rio Detalhado
```bash
python evaluate.py \
  --provider openai \
  --model gpt-3.5-turbo \
  --api_key sk-... \
  --dataset_path meu_dataset.json \
  --csv_out resultados_gpt35.csv \
  --detailed_report \
  --limit 50 \
  --parallelism 10
```

## ğŸ” Formato de Resposta Esperado

O modelo deve responder seguindo o formato:
```
[ExplicaÃ§Ã£o opcional do modelo]

Resposta: Verdadeiro
```
ou
```
[ExplicaÃ§Ã£o opcional do modelo]

Resposta: Falso
```

O sistema extrai automaticamente a Ãºltima ocorrÃªncia de "Verdadeiro" ou "Falso" na resposta.

## ğŸ“¤ SaÃ­das

### 1. Console
- Progresso em tempo real com barra de progresso
- AcurÃ¡cia parcial durante execuÃ§Ã£o
- Resumo final com mÃ©tricas completas

### 2. Arquivo CSV (`resultados_avaliacao.csv`)
ContÃ©m as seguintes colunas:
- `arquivo`: Nome do arquivo fonte
- `titulo`: TÃ­tulo do conjunto
- `idx_local`: Ãndice da pergunta
- `pergunta`: Texto da pergunta
- `esperado`: Resposta correta
- `pred`: PrediÃ§Ã£o do modelo
- `correta`: 1 se acertou, 0 se errou
- `resposta_bruta`: Resposta completa do modelo

### 3. RelatÃ³rio JSON Detalhado (opcional)
Quando usar `--detailed_report`, gera arquivo JSON com:
- Timestamp da avaliaÃ§Ã£o
- MÃ©tricas agregadas
- MÃ©tricas por arquivo
- MÃ©tricas por tÃ­tulo
- Resultados completos de cada pergunta

## âš¡ ConfiguraÃ§Ã£o de Performance

### Paralelismo
Ajuste `--parallelism` baseado no provider:
- **OpenAI/Maritaca**: 5-10 (respeitar rate limits)
- **Ollama (local)**: 20-50 (depende do hardware)
- **AWS Bedrock**: 10-20 (verificar cotas da regiÃ£o)

### Timeout
Ajuste `--timeout` para modelos mais lentos ou prompts complexos

## ğŸ”§ Troubleshooting

### Erro de ImportaÃ§Ã£o
```bash
ImportError: No module named 'langchain'
```
**SoluÃ§Ã£o**: Instalar dependÃªncias
```bash
pip install -r requirements.txt
```

### Erro de API Key
```bash
Erro: --api_key Ã© obrigatÃ³rio para o provider openai
```
**SoluÃ§Ã£o**: Fornecer a chave de API apropriada

### Timeout em Modelos Locais
**SoluÃ§Ã£o**: Aumentar o timeout
```bash
--timeout 300  # 5 minutos
```

### Rate Limit Exceeded
**SoluÃ§Ã£o**: Reduzir paralelismo
```bash
--parallelism 3
```

## ğŸ¤ ContribuiÃ§Ã£o

Para contribuir com o projeto:

1. Adicionar novos providers em `src/providers/`
2. Estender a classe `BaseLLMProvider`
3. Implementar o mÃ©todo `initialize()`
4. Atualizar `evaluate.py` para incluir o novo provider
5. Adicionar dependÃªncias em `requirements.txt`
6. Documentar uso no README

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a especificada no arquivo [LICENSE](LICENSE).

## ğŸ”„ MigraÃ§Ã£o do Script Antigo

Se vocÃª estava usando o script `evaluate_py.py` anterior, migre para o novo sistema:

**Antes (antigo):**
```bash
python evaluate_py.py \
  --model sabia3.1 \
  --base_url https://api.maritaca.ai/v1 \
  --api_key KEY
```

**Agora (novo):**
```bash
python evaluate.py \
  --provider maritaca \
  --model sabia3.1 \
  --api_key KEY
```

### Principais Melhorias
- âœ… Suporte a mÃºltiplos providers
- âœ… Arquitetura modular e extensÃ­vel
- âœ… IntegraÃ§Ã£o com LangChain
- âœ… Melhor tratamento de erros
- âœ… RelatÃ³rios mais detalhados
- âœ… CÃ³digo mais limpo e manutenÃ­vel

## ğŸ‘¥ Autores

Desenvolvido para avaliaÃ§Ã£o de LLMs em portuguÃªs brasileiro no contexto mÃ©dico e de saÃºde.